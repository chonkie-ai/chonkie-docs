---
title: 'SentenceChunker'
description: 'Split text into chunks while preserving sentence boundaries'
icon: 'align-left'
---

The `SentenceChunker` splits text into chunks while preserving complete sentences, ensuring that each chunk maintains proper sentence boundaries and context.

## Installation

SentenceChunker is included in the base installation of Chonkie. No additional dependencies are required.

<Info>For installation instructions, see the [Installation Guide](/getting-started/installation).</Info>

## Initialization

```python
from chonkie import SentenceChunker

# Basic initialization with default parameters
chunker = SentenceChunker(
    tokenizer_or_token_counter="gpt2",                # Supports string identifiers
    chunk_size=512,                                   # Maximum tokens per chunk
    chunk_overlap=0,                                  # Overlap between chunks
    min_sentences_per_chunk=1,                        # Minimum sentences in each chunk
    min_characters_per_sentence=12,                   # Minimum characters per sentence
    approximate=True,                                 # Use approximate token counting
    delim=[".", "?", "!", "\n"],                      # Delimiters to use for chunking
    include_delim="prev",                             # Include the delimiter in the chunk
    return_type="chunks"                              # Return Chunks or texts only 
)
```

## Parameters

<ParamField
    path="tokenizer_or_token_counter"
    type="Union[str, tokenizers.Tokenizer, tiktoken.Encoding, Callable]"
    default="gpt2"
>
    Tokenizer to use. Can be a string identifier or a tokenizer instance.
    If a callable is passed, it should take a string and return the number of tokens in the string.
    See the [Universal Tokenizer Support](/chunkers/overview#universal-tokenizer-support) section for more details.
</ParamField>

<ParamField
    path="chunk_size"
    type="int"
    default="512"
>
    Maximum number of tokens per chunk
</ParamField>

<ParamField
    path="chunk_overlap"
    type="int"
    default="0"
>
    Number of overlapping tokens between chunks
</ParamField>

<ParamField
    path="min_sentences_per_chunk"
    type="int"
    default="1"
>
    Minimum number of sentences to include in each chunk.
    In case it is not possible to meet this requirement, the chunker will return as many sentences as possible.
</ParamField>

<ParamField
    path="min_characters_per_sentence"
    type="Optional[int]"
    default="None"
>
    Minimum number of characters per sentence.
    In case it is not possible to meet this requirement, the chunker will return as many characters as possible.
</ParamField>

<ParamField
    path="approximate"
    type="bool"
    default="True"
>
    Use approximate token counting
</ParamField>

<ParamField
    path="delim"
    type="Union[str, List[str]]"
    default="['.', '?', '!', '\n']"
>
    Delimiters to use for chunking.
    If a string is passed, it will be converted to a list.
</ParamField>

<ParamField
    path="include_delim"
    type="Literal['prev', 'next'] | None"
    default="prev"
>
    Whether to include the delimiter in the chunk.
    If 'prev', the delimiter will be included at the end of the previous chunk.
    If 'next', the delimiter will be included at the beginning of the next chunk.
    If None, the delimiter will not be included in the chunk.
</ParamField>

<ParamField
    path="return_type"
    type="Literal['chunks', 'texts']"
    default="chunks"
>
    Return type. Can be `chunks` or `texts`.
</ParamField>

## Methods

The `SentenceChunker` class provides the following methods:

### `__call__`

The `__call__` method can be used to call the chunker on a single text or a list of texts. It returns a list of `SentenceChunk` objects or a list of strings, depending on the `return_type` parameter.


**Arguments**

<ParamField
    path="text"
    type="Union[str, List[str]]"
>
    Text to chunk.
</ParamField>

<ParamField
    path="show_progress_bar"
    type="bool"
    default="True"
>
    Whether to show a progress bar.
</ParamField>

**Returns**

<ResponseField
    name="returns"
    type="Union[List[SentenceChunk], List[List[SentenceChunk]], List[str], List[List[str]]]"
>
    List of `SentenceChunk` objects or a list of lists of `SentenceChunk` objects or a list of strings or a list of lists of strings.
    The structure of the returned list depends on the `return_type` parameter and the amount of input texts.
</ResponseField>

### `.chunk`

Chunk a single text.

**Arguments**

<ParamField
    path="text"
    type="str"
>
    Text to chunk.
</ParamField>

**Returns**

<ResponseField
    name="returns"
    type="Union[List[SentenceChunk], List[str]]"
>
    Chunked text.
</ResponseField>

### `.chunk_batch`

Chunk a batch of texts.

**Arguments**

<ParamField
    path="texts"
    type="List[str]"
>
    List of texts to chunk.
</ParamField>

**Returns**

<ResponseField
    name="returns"
    type="Union[List[List[SentenceChunk]], List[List[str]]]"
>
    List of lists of `SentenceChunk` objects or a list of lists of strings.
</ResponseField>

## Usage Examples

<AccordionGroup>
    <Accordion title="Basic Usage">
        ```python
        # Import the SentenceChunker
        from chonkie import SentenceChunker

        # Initialize the chunker
        chunker = SentenceChunker()

        # Chunk a single text
        chunks = chunker("Some long text that needs to be chunked into smaller pieces...")

        # Print the chunks
        for chunk in chunks:
            print(chunk.text)
            print(chunk.token_count)
            print(chunk.sentences)
            print(f"{chunk.start_index} - {chunk.end_index}")
        ```
    </Accordion>

    <Accordion title="Getting texts instead of chunks">
    You can also get texts instead of chunks by passing in "texts" in the `return_type` argument.
    
        ```python
        # Import the SentenceChunker
        from chonkie import SentenceChunker

        # Initialize the chunker
        chunker = SentenceChunker(return_type="texts")

        # Chunk a single text
        chunks = chunker("Some long text that needs to be chunked into smaller pieces...")

        # Print the chunks
        for chunk in chunks:
            print(chunk) # This will print the chunk text (str)
        ```
    </Accordion>

    <Accordion title="Batch Processing">
        You can also process a batch of texts at once.
        
        ```python
        # Import the SentenceChunker
        from chonkie import SentenceChunker

        # Initialize the chunker
        chunker = SentenceChunker()

        # Chunk a batch of texts
        # You can directly call the chunker OR use the `.chunk_batch()` method
        batch_chunks = chunker(["Text 1", "Text 2", "Text 3"], show_progress_bar=True)
        batch_chunks = chunker.chunk_batch(["Text 1", "Text 2", "Text 3"], show_progress_bar=False)

        # Print the chunks
        for chunks in batch_chunks:
            for chunk in chunks:
                print(chunk.text)
                print(chunk.token_count)
                print(chunk.sentences)
                print(f"{chunk.start_index} - {chunk.end_index}")
        ```
    </Accordion>

    <Accordion title="Using a custom tokenizer">
        You can also use a custom tokenizer by passing in a tokenizer instance.
        
        ```python
        # Import the SentenceChunker and Tokenizer
        from chonkie import SentenceChunker
        from tokenizers import Tokenizer

        # Initialize the tokenizer
        tokenizer = Tokenizer.from_pretrained("bert-base-uncased") # or any other tokenizer

        # Initialize the chunker
        chunker = SentenceChunker(tokenizer_or_token_counter="bert-base-uncased") # Directly pass the tokenizer name
        chunker = SentenceChunker(tokenizer_or_token_counter=tokenizer) # Pass the tokenizer instance

        # Chunk a single text
        chunks = chunker("Some long text that needs to be chunked into smaller pieces...")

        # Print the chunks
        for chunk in chunks:
            print(chunk.text)
            print(chunk.token_count)
            print(chunk.sentences)
            print(f"{chunk.start_index} - {chunk.end_index}")
        ```
    </Accordion>

    <Accordion title="Using a custom token counter">
        You can also use a custom token counter by passing in a callable.
        
        ```python
        # Import the SentenceChunker
        from chonkie import SentenceChunker

        # Define a custom token counter
        def custom_token_counter(text):
            return len(text)

        # Initialize the chunker
        chunker = SentenceChunker(tokenizer_or_token_counter=custom_token_counter)
        chunker = SentenceChunker(tokenizer_or_token_counter=lambda x: len(x)) # Equivalent to the custom token counter

        # Chunk a single text
        chunks = chunker("Some long text that needs to be chunked into smaller pieces...")

        # Print the chunks
        for chunk in chunks:
            print(chunk.text)
            print(chunk.token_count)
            print(chunk.sentences)
            print(f"{chunk.start_index} - {chunk.end_index}")
        ```
    </Accordion>

    <Accordion title="Using custom delimiters">
        You can also use custom delimiters by passing in a list of delimiters.
        This is especially useful when you want to chunk text in a language other than English, like Chinese or Hindi. 
        The default delimiters are `[".", "?", "!", "\n"]`.

        ```python
        # Import the SentenceChunker
        from chonkie import SentenceChunker

        # Initialize the chunker
        chunker = SentenceChunker(delim="。", include_delim="prev")
        chunker = SentenceChunker(delim=["。", "？", "！", "\n"], include_delim="prev")

        # Chunk a single text
        chunks = chunker("Some long text that needs to be chunked into smaller pieces...")

        # Print the chunks
        for chunk in chunks:
            print(chunk.text)
            print(chunk.token_count)
            print(chunk.sentences)
            print(f"{chunk.start_index} - {chunk.end_index}")
        ```
    </Accordion>
</AccordionGroup>

## Associated Return Types

SentenceChunker returns chunks as `SentenceChunk` objects with additional sentence metadata:

```python
@dataclass
class Sentence:
    text: str           # The sentence text
    start_index: int    # Starting position in original text
    end_index: int      # Ending position in original text
    token_count: int    # Number of tokens in sentence

@dataclass
class SentenceChunk(Chunk):
    text: str           # The chunk text
    start_index: int    # Starting position in original text
    end_index: int      # Ending position in original text
    token_count: int    # Number of tokens in chunk
    sentences: List[Sentence]  # List of sentences in chunk
```